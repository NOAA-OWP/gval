{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1702330",
   "metadata": {},
   "source": [
    "<img src=\"gval_light_mode.png\" style=\"float:left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fa8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray as rxr\n",
    "import gval\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14713f5",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64da5e7b",
   "metadata": {},
   "source": [
    "It is preferred to use masking and scaling by default.  If your original data does not have nodata or does not have nodata assigned, please assign using: `rio.set_nodata(<your_nodata_value>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91c0b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate = rxr.open_rasterio('candidate_map_two_class_categorical.tif', mask_and_scale=True)\n",
    "benchmark = rxr.open_rasterio('benchmark_map_two_class_categorical.tif', mask_and_scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d496084",
   "metadata": {},
   "source": [
    "## Run GVAL Categorical Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d293073",
   "metadata": {},
   "source": [
    "An example of running the entire process with one command using minimal arguments is deomnstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541857a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_map, crosstab_table, metric_table = candidate.gval.categorical_compare(benchmark,\n",
    "                                                                                 positive_categories=[2],\n",
    "                                                                                 negative_categories=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6555af46",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eaeeea",
   "metadata": {},
   "source": [
    "#### Agreement Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24dfc06",
   "metadata": {},
   "source": [
    "The agreement map compares the encodings of the benchmark map and candidate map using a \"comparison function\" to then output unique encodings.  In this particular case the \"Szudzik\" comparison function was used by default since no argument was passed in for the `comparison_function` argument.  The Szudzik function is defined below:\n",
    "\n",
    "$\n",
    "c = \\text{candidate value} \\\\\n",
    "b = \\text{benchmark value} \\\\\n",
    "f(x)= \n",
    "\\begin{cases}\n",
    "    c^{2} + c + b,& \\text{if } c\\geq b\\\\\n",
    "    b^{2} + c,              & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "\n",
    "The resulting map allows a user to visualize these encodings as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef13a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_map.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9c9c19",
   "metadata": {},
   "source": [
    "#### Cross-tabulation Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ca1f3",
   "metadata": {},
   "source": [
    "A cross-tabulation table displays the frequency of each class in the presence of another within the spatial unit of interest. (In this case a pixel in each raster dataset.)  This can then be used to compute categorical statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc9df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b000f2",
   "metadata": {},
   "source": [
    "#### Metric Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e5d48d",
   "metadata": {},
   "source": [
    "A metric table contains information about the unit of analysis, (a single band in this case), and selected categorical statistics.  This is done by specifying the positive and negative categories of each dataset and then choosing the statistics of interest.  Since we did not provide the `metrics` argument GVAL computed all of the available categorical statistics.  (<b>Note: if there is no negative class encoding all statistics computing negatives will be skipped.</b>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cb3626",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3eb3af",
   "metadata": {},
   "source": [
    "## Access to Individual GVAL Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf6a67",
   "metadata": {},
   "source": [
    "Aside form running the entire process, it is possible to run each of the following steps individually: spatial alignment, computing an agreement map, computing a cross-tabulation table, and computing a metric table. This allows for flexibility in workflows so that a user may use as much or as little functionality as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7c6d3f",
   "metadata": {},
   "source": [
    "### Spatial Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6070e8",
   "metadata": {},
   "source": [
    "Spatial alignment by default aligns to the benchmark map, however one can also align to the candidate map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7264ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate, benchmark = candidate.gval.spatial_alignment(benchmark_map=benchmark,\n",
    "                                                        target_map = \"candidate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a4bd1a",
   "metadata": {},
   "source": [
    "Or to another alternate map altogether:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3917e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_map = rxr.open_rasterio('target_map_two_class_categorical.tif')\n",
    "candidate, benchmark = candidate.gval.spatial_alignment(benchmark_map=benchmark,\n",
    "                                                        target_map = target_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686cdd37",
   "metadata": {},
   "source": [
    "The default is to resample using the \"nearest\" method.  Although not applicable for this case of categorical comparisons, one can change the `resampling` argument to use alternative resampling methods such as bilinear or cubic resampling.  These methods would be relevant in the case of continuous datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376c8a9",
   "metadata": {},
   "source": [
    "### Agreement Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae6d51",
   "metadata": {},
   "source": [
    "The \"szudzik\" comparison function is run by default if the `comparison_function` argument is not provided, but one may use the \"cantor\" pairing function, the \"pairing_dict\" function, or a custom callable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e3c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_map = candidate.gval.compute_agreement_map(benchmark_map=benchmark, \n",
    "                                                     comparison_function='cantor')\n",
    "\n",
    "agreement_map.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9adb0c2d",
   "metadata": {},
   "source": [
    "#### Pairing Dictionary\n",
    "\n",
    "A pairing dictionary can be provided by the user to allow for more control when specifying the agreement value outputs. \n",
    "\n",
    "A pairing dictionary has keys that are tuples corresponding to every unique combination of values in the candidate and benchmark, respectively. The values represent the agreement values for each combination. An example pairing dictionary for the candidate values `[1,2]` and benchmark values `[0, 2]` is provided below. A user is currently responsible for including the NoDataValue in the pairing dictionary which in the masked case is `np.nan`. \n",
    "\n",
    "NOTE: Pairing dictionary functionality is currently slow and needs some work to speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614b8d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairing_dict = {\n",
    "    (1, 0) : 0,\n",
    "    (1, 2) : 1, \n",
    "    (2, 0) : 2,\n",
    "    (2, 2) : 3,\n",
    "    (np.nan, 0) : np.nan,\n",
    "    (np.nan, 2) : np.nan,\n",
    "    (1, np.nan) : np.nan,\n",
    "    (2, np.nan) : np.nan\n",
    "}\n",
    "\n",
    "agreement_map = candidate.gval.compute_agreement_map(benchmark_map=benchmark, \n",
    "                                                     pairing_dict = pairing_dict)\n",
    "\n",
    "agreement_map.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b639aeea",
   "metadata": {},
   "source": [
    "Instead of building a pairing dictionary, a user can pass the unique candidate and benchmark values to use and a pairing dictionary will be built for the user. The user should also pass the NoDataValue used within the candidate and benchmark maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d02bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_map = candidate.gval.compute_agreement_map(benchmark_map=benchmark, \n",
    "                                                     comparison_function='pairing_dict',\n",
    "                                                     allow_candidate_values=[np.nan, 1, 2],\n",
    "                                                     allow_benchmark_values=[np.nan, 0, 2])\n",
    "\n",
    "agreement_map.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c5a2d",
   "metadata": {},
   "source": [
    "#### Registration of Custom Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b259d70",
   "metadata": {},
   "source": [
    "In this case we register the arbitrary pairing function `multiply` with the name \"multi\" and then vectorize it.  `Multiply` can also be passed in as a function in the `comparison_function` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972f07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gval import Comparison\n",
    "from numbers import Number\n",
    "\n",
    "@Comparison.register_function(name='multi', vectorize_func=True)\n",
    "def multiply(c: Number, b: Number) -> Number:\n",
    "    return c * b\n",
    "\n",
    "agreement_map = candidate.gval.compute_agreement_map(benchmark_map=benchmark, \n",
    "                                                     comparison_function=\"multi\")\n",
    "\n",
    "agreement_map.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7e8929",
   "metadata": {},
   "source": [
    "A user can also pick which candidate values or benchmark values to use by providing lists to the `allow_candidate_values` and `allow_benchmark_values` arguments.  Finally, a user can choose to write nodata to unmasked datasets with the `nodata` value, or to masked/scaled datasets with `encode_nodata`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5181e51a",
   "metadata": {},
   "source": [
    "### Cross-tabulation Table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3906909f",
   "metadata": {},
   "source": [
    "When computing a crosstab table, a user may create an allow list for candidate/benchmark values just as done in the compute agreement map method.  They may also exclude a nodata value with `exclude_value` in the case that `mask_and_scale` is not applied when loading the original data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab_table_allow = candidate.gval.compute_crosstab(benchmark,\n",
    "                                                       allow_benchmark_values=[0, 2],\n",
    "                                                       allow_candidate_values=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c436ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab_table_allow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d94c67e",
   "metadata": {},
   "source": [
    "#### Metric Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9aa1cc",
   "metadata": {},
   "source": [
    "Although all categorical metrics are computed by default if no argument is provided, `metrics` can also take a list of the desired metrics and will only return metrics in this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba3fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "metric_table_select = crosstab_table.gval.compute_metrics(negative_categories= [0, 1],\n",
    "                                                          positive_categories = [2],\n",
    "                                                          metrics=['true_positive_rate', 'prevalence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04b85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_table_select"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "382b1a13",
   "metadata": {},
   "source": [
    "Just like registering pairing functions, you are able to register a metric function on both a method and a class of functions.  Below is registering a metric funciton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67938408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gval import CatStats\n",
    "\n",
    "@CatStats.register_function(name=\"error_balance\", vectorize_func=True)\n",
    "def error_balance(fp: Number, fn: Number) -> float:\n",
    "    return fp / fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf6e16f4",
   "metadata": {},
   "source": [
    "The following is registering a class of metric functions. In this case, the names associated with each function will respond to each method's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8eeb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "@CatStats.register_function_class(vectorize_func=True)\n",
    "class MetricFunctions:\n",
    "    \n",
    "    @staticmethod\n",
    "    def arbitrary1(tp: Number, tn: Number) -> float:\n",
    "        return tp + tn\n",
    "    \n",
    "    @staticmethod\n",
    "    def arbitrary2(tp: Number, tn: Number) -> float:\n",
    "        return tp - tn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75deed2d",
   "metadata": {},
   "source": [
    "All of these functions are now callable as metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a41eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_table_register = crosstab_table.gval.compute_metrics(negative_categories= None,\n",
    "                                                            positive_categories = [2],\n",
    "                                                            metrics=['error_balance', 'arbitrary1', 'arbitrary2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab884b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_table_register"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f930bbd",
   "metadata": {},
   "source": [
    "## Save Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c625d6",
   "metadata": {},
   "source": [
    "Finally, a user can take the results and save them to a directory of their choice.  The following is an example of saving the agreement map and then the metric table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a1da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output agreement map\n",
    "agreement_file = 'agreement_map.tif'\n",
    "metric_file = 'metric_file.csv'\n",
    "\n",
    "agreement_map.rio.to_raster(agreement_file)\n",
    "metric_table.to_csv(metric_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
